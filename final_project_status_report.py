# -*- coding: utf-8 -*-
"""Final_Project_status_Report_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TIzxFVPxq8Ge5zulSPwoAcJWa8lMkV-a

# Project: Build a Project status report using RAG system

## Install OpenAI, and LangChain dependencies
"""

!pip install langchain
!pip install langchain-openai
!pip install langchain-community
!pip install langchain-chroma
import yaml
import os
import openai

"""## Setup Environment Variables"""

with open('/content/sample_data/chatgpt_api_credentials.yml', 'r') as file:
    api_creds = yaml.safe_load(file)
    os.environ['OPENAI_API_KEY'] = api_creds['openai_key']

"""### Open AI Embedding Models

LangChain enables us to access Open AI embedding models which include the newest models: a smaller and highly efficient `text-embedding-3-small` model, and a larger and more powerful `text-embedding-3-large` model.
"""

from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import OpenAIEmbeddings
# details here: https://openai.com/blog/new-embedding-models-and-api-updates
openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-large')

"""### Get the PDF data"""

!pip install PyMuPDF
import os
import fitz  # PyMuPDF
import re
from langchain.schema import Document
from typing import List

def extract_text_from_pdf(pdf_path: str) -> str:
    """Extract all text from a PDF file using PyMuPDF."""
    doc = fitz.open(pdf_path)
    return "\n".join(page.get_text() for page in doc)

def chunk_text_by_section(text: str, source_file: str) -> List[Document]:
    """
    Chunk text into LangChain Documents based on numbered section headings.
    Matches: 1., 2.1, 3.1.2 etc.
    """
    pattern = r"(?m)^(\d+(?:\.\d+)*\.)\s+(.*)"
    matches = list(re.finditer(pattern, text, re.MULTILINE))

    documents = []
    for i, match in enumerate(matches):
        start = match.start()
        end = matches[i+1].start() if i+1 < len(matches) else len(text)
        section_number = match.group(1).strip()
        section_title = match.group(2).strip()
        section_text = text[start:end].strip()

        documents.append(Document(
            page_content=section_text,
            metadata={
                "section_title": section_title,
                "source_file": os.path.basename(source_file)
            }
        ))
    return documents

def process_pdfs(pdf_paths: List[str]) -> List[Document]:
    """Processes multiple PDFs and returns LangChain Documents per section."""
    all_docs = []
    for path in pdf_paths:
        print(f"📄 Processing: {os.path.basename(path)}")
        text = extract_text_from_pdf(path)
        docs = chunk_text_by_section(text, source_file=path)
        print(f"   ➤ {len(docs)} sections extracted")
        all_docs.extend(docs)
    return all_docs


pdf_files = ["/content/sample_data/Business_Requirement_Document.pdf","/content/sample_data/Solution_Design_Document.pdf","/content/sample_data/System_Design_Document.pdf","/content/sample_data/Test_Case_Document_v1.pdf"]

docs = process_pdfs(pdf_files)

# Step 3: Display some chunks
for i, doc in enumerate(docs[:5]):
    print(f"\n--- Document {i+1} ---")
    print(f"File: {doc.metadata['source_file']}")
    print(f"Section: {doc.metadata['section_title']}")
    print(doc.page_content[:3000])

print(docs)

len(docs)



"""### Create LangChain Documents"""

from langchain.docstore.document import Document

docs = [Document(page_content=doc.page_content,
                 metadata=doc.metadata) for doc in docs]

"""### Split larger documents into smaller chunks

### Create a Vector DB and persist on disk

Here we initialize a connection to a Chroma vector DB client, and also we want to save to disk, so we simply initialize the Chroma client and pass the directory where we want the data to be saved to.
"""

!pip install langchain-chroma
from langchain_chroma import Chroma

# create vector DB of docs and embeddings - takes < 30s on Colab
chroma_db = Chroma.from_documents(documents=docs,
                                  collection_name='rag_projectstatus_db',
                                  embedding=openai_embed_model,
                                  # need to set the distance function to cosine else it uses euclidean by default
                                  # check https://docs.trychroma.com/guides#changing-the-distance-function
                                  collection_metadata={"hnsw:space": "cosine"},
                                  persist_directory="./projectstatus_db")

"""### Load Vector DB from disk

This is just to show once you have a vector database on disk you can just load and create a connection to it anytime
"""

# load from disk
chroma_db = Chroma(persist_directory="./projectstatus_db",
                   collection_name='rag_projectstatus_db',
                   embedding_function=openai_embed_model)

chroma_db

"""## Load Connection to LLM

Here we create a connection to ChatGPT to use later in our chains
"""

from langchain_openai import ChatOpenAI

chatgpt = ChatOpenAI(model_name='gpt-4.1', temperature=0)

"""## Chained Retrieval Pipeline

This strategy uses a chain of multiple retrievers sequentially to get to the most relevant documents. The following is the flow

Similarity Retrieval → Compression Filter → Reranker Model Retrieval

![](https://i.imgur.com/KriNRDJ.gif)

"""

!pip install sentence_transformers
import sentence_transformers
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain.retrievers.document_compressors import LLMChainFilter
from langchain.retrievers import ContextualCompressionRetriever

# Retriever 1 - simple cosine distance based retriever
similarity_retriever = chroma_db.as_retriever(search_type="similarity",
                                              search_kwargs={"k": 5})

#  decides which of the initially retrieved documents to filter out and which ones to return
_filter = LLMChainFilter.from_llm(llm=chatgpt)
# Retriever 2 - retrieves the documents similar to query and then applies the filter
compressor_retriever = ContextualCompressionRetriever(
    base_compressor=_filter, base_retriever=similarity_retriever
)

# download an open-source reranker model - BAAI/bge-reranker-v2-m3
reranker = HuggingFaceCrossEncoder(model_name="BAAI/bge-reranker-large")
reranker_compressor = CrossEncoderReranker(model=reranker, top_n=3)
# Retriever 3 - Uses a Reranker model to rerank retrieval results from the previous retriever
final_retriever = ContextualCompressionRetriever(
    base_compressor=reranker_compressor, base_retriever=compressor_retriever
)

query = "What is the purpose of of this solution"
docs = final_retriever.invoke(query)
docs

"""## Build a QA RAG Chain

To build a RAG chain we need a prompt template which instructs the LLM to not answer questions beyond the scope of the retrieved context documents, there are various such prompts out there, we craft one ourselves below
"""

from langchain_core.prompts import ChatPromptTemplate

prompt = """
You are a Tech Lead and Project Evaluator for a RAG-based AI chatbot solution implementation.

You are responsible for reviewing the following documents:
1. Business Requirement Document (BRD)
2. System Design Document
3. Solution Design Document
4. Test Case Document

Follow these instructions strictly:
- Do NOT make up or assume any information.
- Base all answers ONLY on the provided context.
- Follow industry best practices in architecture, security, testing, and delivery.

Your evaluation tasks include:

1. **✅ Business Requirement Validation**
   - Check whether the solution meets all the business goals and in-scope requirements.
   - Highlight any missing or partially fulfilled requirements.

2. **🏗️ System and Solution Design Evaluation**
   - Review architecture components such as Azure OpenAI, Hugging Face, Chroma, ADLS, Synapse, and Power BI.
   - Validate the design against enterprise-grade cloud best practices (modularity, scalability, fault tolerance).
   - Suggest improvements where applicable.

3. **🔒 Security and Access Control Review**
   - Confirm encryption of data at rest and in transit.
   - Validate role-based access (RBAC) through Azure AD or similar controls.
   - Check for any potential data leakage or non-compliance with data protection standards.

4. **🧪 Testing Validation**
   - Review the test case document for coverage across functional, integration, and non-functional (e.g., load, security) testing.
   - Identify any gaps in testing or missing test scenarios.
   - Ensure test outcomes align with business readiness.

5. **🚦 Performance and Scalability Assessment**
   - Evaluate if the system is capable of supporting high user load (e.g., 500+ users/day).
   - Confirm autoscaling, monitoring, and performance optimization strategies.
   - Identify any risks or bottlenecks that may affect production readiness.

6. **📋 Final Evaluation Report**
   Return a well-structured evaluation with the following sections:
   - ✔️ Summary of what is implemented well
   - ⚠️ Gaps, risks, or non-conformities
   - 💡 Recommendations for improvement
   - ✅ Final readiness score or Go/No-Go assessment

---

Question:
{question}

Context:
{context}

Answer:
         """

prompt_template = ChatPromptTemplate.from_template(prompt)

"""## LCEL Syntax for QA RAG Chain - Recommended

Here we show you how to create the RAG chain using LangChain's recommended LCEL
"""

from langchain_core.runnables import RunnablePassthrough

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

qa_rag_chain = (
    {
        "context": (final_retriever
                      |
                    format_docs),
        "question": RunnablePassthrough()
    }
      |
    prompt_template
      |
    chatgpt
)

from IPython.display import Markdown, display

query = "Please evaluate the implementation against business, design, security, performance, and test requirements."
result = qa_rag_chain.invoke(query)
display(Markdown(result.content))

from langchain_core.prompts import ChatPromptTemplate

prompt = """
You are a Solution Architect and Technical Reviewer.

Your task is to review the **System Design Document** of an AI-based solution implementation. The system uses cloud-native architecture and integrates various AI, data, and analytics components.

Please evaluate the following aspects:

🏗️ System and Solution Design Evaluation**
   - Review architecture components such as Azure OpenAI, Hugging Face, Chroma, ADLS, Synapse, and Power BI.
   - Validate the design against enterprise-grade cloud best practices (modularity, scalability, fault tolerance).
   - Suggest improvements where applicable.


Do not fabricate details. Base your review **strictly** on the provided system design documentation.

---

System Design Context:
{context}

Answer:
"""


prompt_template_1 = ChatPromptTemplate.from_template(prompt)

from langchain_core.runnables import RunnablePassthrough

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

qa_rag_chain = (
    {
        "context": (final_retriever
                      |
                    format_docs),
        "question": RunnablePassthrough()
    }
      |
    prompt_template_1
      |
    chatgpt
)

from IPython.display import Markdown, display

query = "Does the technology stack used for chatbot solution is aligned correctly?"
result = qa_rag_chain.invoke(query)
display(Markdown(result.content))

query = "From the system design document review and validate each technical component used in this solution with Azure Techstack and share the report"
result = qa_rag_chain.invoke(query)
display(Markdown(result.content))

from langchain_core.prompts import ChatPromptTemplate

prompt = """
You are a Solution Architect and Technical Reviewer.

Your task is to review the **System Design Document** of an AI- RAG based solution implementation. The system uses cloud-native architecture and integrates various AI, data, and analytics components.

Please evaluate the following aspects:

🏗️ System and Solution Design Evaluation**
   - Review architecture technical components such as Azure OpenAI, Hugging Face, Chroma, ADLS, Synapse, and Power BI.
   - Validate the technical componentsused in this solution against business requirement and enterprise-grade cloud best practices (modularity, scalability, fault tolerance).
   - Suggest improvements where applicable.
   -  Confirm whether the chosen technology is the best fit for its purpose in the solution.

You are a Solution Architect and Technical Reviewer.

Your task is to answer the following question in a strict format with tabular format.

👉 Your response in tabulat format should have:
- Technology Component -Example ADLS,Synapse"
-  Best Fit -Yes or NO
-  Explaination - ADLS is good to storing the chat interactions
- "Suggestions.

Then provide a brief explanation (1–3 sentences), based ONLY on the system design document provided.


Do not fabricate details. Base your review **strictly** on the provided system design documentation.



System Design Context:
{context}

Answer:
"""


prompt_template_2 = ChatPromptTemplate.from_template(prompt)

from langchain_core.runnables import RunnablePassthrough

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

qa_rag_chain = (
    {
        "context": (final_retriever
                      |
                    format_docs),
        "question": RunnablePassthrough()
    }
      |
    prompt_template_2
      |
    chatgpt
)

query = """Evaluate each technical component listed in the system design document for its suitability, scalability, and alignment with Azure best practices.
"""
result = qa_rag_chain.invoke(query)
display(Markdown(result.content))

query = "What are the technical components listed in the system design document"
docs = final_retriever.invoke(query)

from langchain_core.prompts import ChatPromptTemplate

prompt = """
You are a Quality Assurance (QA) Lead and Test Reviewer.

Your task is to review the Test Case Document in the context of the Business Requirement Document (BRD). Based on this, produce a formal QA report that:

1. ✅ **Test Coverage Analysis**
   - Evaluate whether the test cases cover all key business requirements from the BRD.
   - Confirm that all functional, integration, non-functional (performance, security), and edge scenarios have test coverage.
   - Highlight any missing or incomplete test cases.

2. 🧪 **Test Execution Analysis**
   - Review the results of executed test cases (Pass/Fail/Not Executed).
   - Summarize the number and percentage of tests passed, failed, and skipped.
   - Highlight any failed test cases that directly impact critical business flows.

3. 📊 **Comparison with Business Requirements**
   - Cross-map test cases to the stated business goals and validate if each requirement is tested.
   - Identify any requirements that are not tested or insufficiently validated.
   - Call out high-risk gaps or mismatches between the BRD and testing.

4. ⚠️ **Gap Identification & Risk Evaluation**
   - Identify any uncovered use cases, untested integrations, or missing validation steps.
   - Mention risks to production stability or compliance due to incomplete QA.

5. 🚦 **Final Go/No-Go Recommendation**
   - Conclude with a "Go" or "No-Go" release recommendation.
   - Justify the decision clearly based on testing completeness, quality, and alignment with the business goals.

Do not assume or fabricate information. Base your analysis strictly on the provided documents.


Question:
{question}

Context:
{context}
"""


prompt_test_template_1 = ChatPromptTemplate.from_template(prompt)

from langchain_core.runnables import RunnablePassthrough

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

qa_rag_chain = (
    {
        "context": (final_retriever
                      |
                    format_docs),
        "question": RunnablePassthrough()
    }
      |
    prompt_test_template_1
      |
    chatgpt
)

from IPython.display import Markdown, display
query = "Based on the test case document and the business requirements, evaluate Chatbot Functionality, RAG Flow (Embedding, Retrieval, Generation), Integration with Azure OpenAI and Chroma DB, Role-Based Access Control (Azure AD), Logging & Audit Trail, Analytics & Dashboard, CI/CD and Deployment Validation, Non-Functional Testing (Performance, Load, Security) whether the all test cases are covered , and highlight any of the test cases missed to consider in testing, and sufficient for production release. Provide a detailed QA evaluation report with a final Go/No-Go recommendation"
result = qa_rag_chain.invoke(query)
display(Markdown(result.content))

!pip install deepeval
from deepeval.models.base_model import DeepEvalBaseLLM
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval import evaluate
from deepeval.metrics import (
    ContextualPrecisionMetric,
    ContextualRecallMetric,
    ContextualRelevancyMetric,
    AnswerRelevancyMetric,
    FaithfulnessMetric,
    HallucinationMetric,
    GEval,
)

query = "Based on the test case document and the business requirements, evaluate whether the all test cases are covered , and highlight any of the test cases missed to consider in testing, and sufficient for production release. Provide a detailed QA evaluation report with a final Go/No-Go recommendation"
docs = final_retriever.invoke(query)

os.environ["DEEPEVAL_TELEMETRY_OPT_OUT"] = "YES"

from langchain_openai import ChatOpenAI
from deepeval.models.base_model import DeepEvalBaseLLM

class LangChainModelWrapper(DeepEvalBaseLLM):
    def __init__(self, model):
        self.model = model

    def generate(self, prompt: str, **kwargs) -> str:
        return self.model.invoke(prompt).content

    async def a_generate(self, prompt: str, **kwargs) -> str:
        # fallback: call sync method for compatibility
        return self.generate(prompt)

    def get_model_name(self) -> str:
        return "LangChain-ChatOpenAI"

    def load_model(self):
        pass

query = "Based on the test case document and the business requirements, evaluate whether the all test cases are covered , and highlight any of the test cases missed to consider in testing, and sufficient for production release. Provide a detailed QA evaluation report with a final Go/No-Go recommendation"

human_answer = """As per the business requirement  Auditing module ,data in report and database validations,integration testing and performance scenarios are not coved in detail"""

test_case = LLMTestCase(
    input=query,
    actual_output=qa_rag_chain.invoke(query).content,
    context=[human_answer]
)

metric = HallucinationMetric(
    threshold=0.6,
    model="gpt-4-turbo",               # ✅ let DeepEval handle the model directly
    include_reason=True,
    verbose_mode=True
)

result = evaluate([test_case], [metric])

print('Sucess:', result.test_results[0].metrics_data[0].success)
print('Score:', result.test_results[0].metrics_data[0].score)
print('Reason:', result.test_results[0].metrics_data[0].reason)

os.environ["OPENAI_API_KEY"] = api_creds['openai_key']

metric = AnswerRelevancyMetric(
    threshold=0.6,
    model="gpt-4-turbo",               # ✅ let DeepEval handle the model directly
    include_reason=True,
    verbose_mode=True
)

test_case = LLMTestCase(
    input=query,
    actual_output=qa_rag_chain.invoke(query).content,
)

result = evaluate([test_case], [metric])

print('Sucess:', result.test_results[0].metrics_data[0].success)
print('Score:', result.test_results[0].metrics_data[0].score)
print('Reason:', result.test_results[0].metrics_data[0].reason)

# Your LangChain model
api_key=api_creds['openai_key']
chatgpt = ChatOpenAI(model="gpt-4", api_key=api_key)

# Wrap it
wrapped_model = LangChainModelWrapper(chatgpt)

test_case = LLMTestCase(
    input=query,
    actual_output=qa_rag_chain.invoke(query).content,
)

metric = AnswerRelevancyMetric(
    threshold=0.6,
    model=wrapped_model,
    include_reason=True,
    verbose_mode=True,
    async_mode=False
)

result = evaluate([test_case], [metric])

test_case = LLMTestCase(
    input=query,
    actual_output=qa_rag_chain.invoke(query),
)

metric = AnswerRelevancyMetric(
    threshold=0.6,
    model=chatgpt,
    include_reason=True,
    verbose_mode=True
)

result = evaluate([test_case], [metric])